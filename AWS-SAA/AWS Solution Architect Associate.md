# AWS Solution Architect Associate

## Key Management Server `KMS`

- `KMS` is a Regional & Public service
  - Every region is isolated when using `KMS` (Separate product in each region)
- Lets you create, store and manage cryptographic keys
  - Handles both Symmetric and Asymmetric keys
  - Capable of performing cryptographic operations (Encryption & Decryption)
- Keys are locked inside of KMS  (They will never leave it) and are securely stored
  - KMS is FIPS 140-2(L2) Certified
- The primary thing KMS manages are Customer Master Key `CMK`
  - These are logical (container for the physical master key)
    - Contains : Key ID, Creation Date, Key Policy (Resource Policy), Description, State (Enabled Disabled)
    - Every CMK is backed by a physical key material, this data is held by KMS and is used to encrypt & decrypt
      - This can be generated by `KMS` or imported if you have an existing one
    - CMKs can only be used to directly encrypt or decrypt data up to 4KB in size
- Data Encryption Keys `DEKs`
  - Data Encryption Key Work flow :
    1. Key is generated and you get two versions of the key
       - Plaintext
       - Cipher text (Encrypted) version of they
         - This is encrypted by the customer master key so it can decrypted by KMS
    2. Data is encrypted using the plaintext version of the key
    3. Discard the plain text version of the key
    4. Store encrypted key with the data
    5. When the data has to be decrypted, you give the encrypted key to KMS to decrypt it using the Customer Master key`
  - These can be generated in KMS and are generated using the Customer Master Key
    - These can be used to decrypt and encrypt data larger that 4KB
  - These are not stored by `KMS` it provides the key to you (or the service using it) then it is discarded
    - This is done because `KMS` does not do the encryption of the data instead you or the service does it
    - **KMS DOES NOT USE THE DATA ENCRYPTION KEY IT ONLY PROVIDES IT**

## Simple Storage Service (S3)

### Object Versioning

This is controlled at the bucket level and is disabled by default, once it is enabled it can never be disabled but can be suspended and reenabled.

- Space is consumed by all versions of the object
- If you turn on versioning on a bucket with a 5GB  file and you have 5 versions you are paying 5GB x 5 for storage

Versioning allows you to store multiple version of an object in a bucket, operations that modify objects generate a new version.

When versioning is disabled the object in the bucket is identified by its key and value, and the id of the object is set to null. Once you enable versioning the ID is changed when the object is modified

- These objects can be referenced by their version id to interact directly or omit this to reference the latest version of an object

- Objects do not get deleted  instead object deletion markers are put in place to  hide the objects.

- The delete marker is a special version of an object that hides all previous version of that object
- The delete marker can be deleted which essentially un-deletes the object.
- Versions of an object can be fully deleted, to do this delete a object and specify the ID of what you want to use.

### MFA Delete

MFA Delete is enabled in the versioning configuration and when it is enabled MFA is required to change bucket versioning state or to delete versions.

- When using MFA Delete with a API call you need to concatenate  the serial number of your MFA  token and the code it generates

### S3 Performance Optimization

By default when you upload a file to S3 it is uploaded as a single blob of data in a single stream

### Single PUT upload

- The file becomes an object and uses the PutObject API call to upload the file the bucket
  - If the stream fails the upload fails and you need to fully restart the upload
  - Speed & Reliability is limited because it is only using one stream of data
  - A single PUT upload is limited to Gb/s

### Multipart Upload

- Multipart upload is when the data is broken up into smaller parts
  - The minimum data size for multipart is 100MB
  - Almost always worth it to use multipart if the file is above 100MB
  - Can be split into a maximum of 10,000 parts 5MB > 5GV
    - The last part can be smaller than 5MB
  - In multipart each part can fail and be restarted instead of the entire upload being restarted

### S3 Transfer Acceleration

This is disabled by default, but when it is enabled S3 uses AWS edge location.

- When data is uploaded to S3 instead of it going directly to the S3 Bucket taking the public internet it instead goes directly to the closest and best performing AWS edge location, this data then travels the AWS Global network which is directly controlled by AWS
  - Goes over fewer "Normal Networks" which slow it down as AWS cant control all ISP networks.
  - The benefits achieved using Transfer Acceleration improve with the father away the uploader is from the S3 Bucket

### S3 Object Storage Classes

- **S3 Standard**
  - This is the default storage class when using S3
  - Should be used for `frequently` accessed data which is `important and non replaceable`
  - Objects are replicated across at least 3 availability zones
  - Provides 11 9's of durability (Lose one object every 10,000 years)
  - Billed GB/Month fee for data stored a $ per GB charge for transfer OUT (IN is free) and a price per 100 requests
    - No specific fee, no minimum duration, no minimum size
- **S3 Standard Infrequent Access** `S3 Standard-IA`
  - Should be used for `long-lived data` which is `important` but `access is infrequent`
  - Objects are replicated across at least 3 availability zones
  - Provides 11 9's of durability (Lose one object every 10,000 years)
  - Pricing model is the same as S3 Standard however it is much cheaper
    - There is now a retrieval fee (Per GB transferred out)
    - Has a minimum duration charge of 30 days and minimum capacity charge of 128KB per object
- **S3 One Zone Infrequent Access** `S3 One Zone-IA`
  - Should be used for `long lived data` which is `non-critical and replaceable` which is accessed `infrequently`  
  - Data stored here is only stored in one availability zone in the region (There is no replication)
  - Cheaper than S3 Standard & S3 Standard-IA
  - Pricing model is the same as S3 Standard however it is much cheaper
    - There is now a retrieval fee (Per GB transferred out)
    - Has a minimum duration charge of 30 days and minimum capacity charge of 128KB per object
- **S3 Glacier**
  - Should be used for `archival data` where `frequent or realtime access is not needed`
  - Replicated across 3 availability zones
  - Provides 11 9's of durability
  - Storage cost that is about 1/5th of S3 Standard
    - Objects in S3 Glacier cannot be made publicly accessible
    - If you want to retrieve an object from Glacier you must initiate a retrieval process (This is something you pay for)
      - Expedited - Data is accessible in 1-5 minutes
      - Standard Data is accessible in 3-5 hours
      - Bulk data is available in 5-12 hours
  - 40KB minimum billable size, 90 day minimum duration charge
  - First byte latency of minutes or hours
- **S3 Glacier Deep Archive**
  - `Archival data` that `rarely if ever needs to be accessed` - hours or days for retrieval
  Replicated across 3 availability zones
  - Provides 11 9's of durability
  - Approx. 1/4 the price of glacier
    - 40KB minimum billable size, 180 days minimum billable duration
  - If you want to retrieve an object from Glacier Archive you must initiate a retrieval process (This is something you pay for)
    - Standard Data is accessible in 12 hours
    - Bulk data is available in up to 48 hours
- **S3 Intelligent-Tiering**
  - Should be used for `long-living data` with `changing` or `unknown` patterns
  - This is a storage class that contains 4 different tiers of storage
    - Frequent Access Tier (S3 Standard)
    - Infrequent Access Tier (S3 Standard-IA)
    - Archive (Glacial)
    - Deep Archive (Glacial Archive)
  - The object is monitored and is moved between tiers as the usage goes up or down
    - Moves any object not accessed for 30 days to `infrequent access` and eventually to `archive` or `deep archive`
    - As objects are accessed they are moved back to the `frequent access` tier, there are no retrieval fees for accessing objects only a 30 day minimum duration

### S3 Select and Glacier Select

- S3 and Glacier select allow you to use a SQL-Like statement to retrieve partial objects from S3 and Glacier
  - You create a cut down SQL statement and send it to the select service, this statement is then used to select part of the object and it is then sent to the client in a pre-filtered way
  - Allows you to operate on many file formats :
    - CSV, JSON, Parquet, BZIP2 Compression for CSV and JSON files

- If you have an app in-taking all the objects in an S3 bucket then filtering on the app side:
  - S3 filtering occurs in-app which retrieves all the objects in a bucket all of which you are billed for
- if you have an app in-taking data but it is being filtered through S3 select:
  - SQL-like expression provided to S3 select only the selected files are transferred and billed  
  - Increase speed the data is received as fewer objects are transferred

### S3 Life Cycle Configuration

- Life cycle rules allows you to automatically transition or expire objects in a bucket
  - Allows you to optimize cost for larger S3 Buckets
- Life Cycle configurations are a set of rules you apply to a bucket or groups of objects
  - These rules consist of actions (If X do Y)
    - **Transition Actions** : Change the affected objects storage class (Transition S3 Standard > Glacier)
      - Smaller objects can cost more (Some storage classes have minimum size)
      - An object needs to stay on S3 Standard for 30 days before transitioning to either of the infrequent access tiers
    - Expiration Actions : Delete the affected objects (Object older than 120 days > Delete)

### S3 Replication

- S3 Allows you to configure the replication of objects between a SOURCE and DESTINATIOn bucket in the same or different AWS Account
- There are two types of replication :
  - **Cross Region Replication** `CRR`
    - Allows the replication between a SOURCE and DESTINATION across different AWS Regions
  - **Same Region Replication** `SRR`
    - Allows the replication between a SOURCE and DESTINATION across the same AWS region
- A **Replication Configuration** is applied to the source bucket this configures S3 to replicate from the source bucket to the destination bucket
  - It also defines logically the destination bucket to use
  - IAM Role to use for the replication process
    - This role is configured to allow the S3 bucket to assume it (Defined in its trust policy)
    - Gives it permission to read objects in the source and replicate those objects to the destination
    - When replicating across different AWS accounts this role is not trusted by the destination account by default, you will need to add a bucket policy to the destination bucket allowing the role to replicate to that bucket.
- S3 Additional Replication Options
  - Replicate all objects in a bucket or just a subset
    - The subset is defined by a filter of some sort
  - What storage class to replicate to
    - The default is to maintain the same storage class
  - Ownership of the object
    - The default is the source account
  - Replication Time Control `RTC`
    - Adds a guaranteed 15 minute replication SLA on this process
    - Without replication is a best effort process
  - Replication Metrics and Notifications
    - Monitor the progress of your replication rule through Cloud-watch Metrics
  - Delete Marker Replication
    - Delete markers created by S3 delete operations will be replicated (Deletions in the source are replicated to the destination)
- S3 Replication Considerations
  - `Not retroactive` & Versioning needs to be `enabled`
    - If you enable replication on a bucket that already has objects the old objects `will not` be replicated
  - `One way replication process` Source > Destination (Not bidirectional)
  - You are able to duplicate un-encrypted objects, objects encrypted with SSE-S3 & SSE-KMS(with extra config)
    - Cannot replicate objects encrypted with SSE-C as S3 does not have the keys used to access the plain text version of the object
  - Source bucket owner needs permissions to the objects that are replicating
  - does not replicate `System Events`, `Glacier` or `Glacier Deep Archive` storage classes
  - Deletes are not replicated
- Why use Replication
  - **Same Region Replication** `SRR`
    -Log Aggregation (Multiple S3 Buckets that receive logs replicate to one central bucket)
    - PROD to TEST Sync
    - Resilience with strict sovereignty requirements
  - **Cross Region Replication** `CRR`
    - Global Resilience Improvements
    - Latency Reduction

### S3 Pre-signed URLs

Pre-Signed URL's are a feature of S3 which allows you to provide another person or application access to an object inside an S3 bucket using your credentials in a safe and secure way.

- How are pre-signed URL's created :
  - a user with access rights to a S3 Bucket sends a request to S3 to generate a pre-signed URL when they make this request the user has to provide the following :
    - Security credentials
    - Bucket Name & Object Key
    - Expiry date and time
    - How to object will be accessed (PUT/GET Operation)
- Once the URL is used the holder is interacting with the object in the bucket as the user who generated it
- You can create a pre-signed URL for an object you have no access too
- When using a pre-signed URL it matches the permissions of the identity that generated it at the point you use it
  - If you get an access denied error when using a pre-signed URL the generating ID could had never had access or does not now
- **Do not** generate a pre-signed URL with a role ; URL stops working when temporary credentials expire you should always use long term identities

### S3 Events Notifications

This is a feature that allows you receive notifications when certain events happen in your bucket.

- Notifications can be delivered to :
  - `SNS` topics, `SQS` queues and Lambda functions
- S3 can alert on various events
  - Object creation (`PUT`,`POST`,`COPY`,Completed Multi Part Uploads)
  - Object Deletion (Delete,DeleteMarkerCreated)
  - Object Restore for objects in Glacier or Glacier Deep Archive (When a restore is started or when it is completed)
  - Object Replication (OperationMissedThreshold, Replication start/fail,etc.)

### S3 Access Logs

- A feature in S3 that provides detailed records for the requests that are made to a bucket
  - Log files consist of `log records` records are `newline-delimited`
    - These records contain `attributes` which contains things such as date/time,the requester, the operation,etc. (these are `space delimited`)

## Virtual Private Cloud (VPC)

### VPC Sizing and Structure

- VPC Considerations
  - What size should the VPC be
    - Minimum size of `/28`(16 IPs), Maximum size of `/16` (65456 IPs)
  - Are there any networks we cannot use
    - Do not overlap / duplicate IP ranges
    - Avoid common ranges to avoid future issues
  - VPC's,Cloud, On-Premises, Partners & Vendors
    - Avoid ranges other parties use (that you think you would interact with)
  - Try to predict the future
  - VPC Structure - Tiers & Resiliency (Availability) Zones
    - Subnets are available in 1 availability zone
  - You can optionally assign a IPv6 by using a `/56 CIDR` block
    - Either AWS will provide the range to you, or you can use an IPv6 range that you own

### Custom VPCs & VPC Theory

- VPCs are regionally isolated and regionally resilient service
  - It is created in a region and operates from all AZs in that region
- Allows you to create isolated networks inside AWS
  - Even in a single region in an account you can have multiple isolated networks
- Nothing is allowed IN or OUT without explicit configuration
- Flexible configuration Simple or Multi-tier
- Hybrid networking
  - Allows you to connect  your network to other cloud providers or to your on-premise networks
- `Default` or `Dedicated Tenancy`
  - Controls weather the services provisioned inside of an VPC are provisioned on shared hardware or dedicated hardware
  - `Default` :
    - Allows you to choose on a per-resource when you provision resources
  - `Dedicated Tenancy` :
    - Any resource created within the VPC HAVE to be on dedicated hardware
- VPCs have a fully featured DNS provided by Route 53
  - its available on the VPC `Base IP +2` address
    - If you have a base IP of `10.0.0.0` the DNS IP would be `10.0.0.2`
    - Has two important settings :
      - `EnableDnsHostnames` - gives instances DNS Names
        - Defines if instances with a public IP are given public DNS names
      - `EnableDnsSupport` - enables DNS resolution in VPC
        - If enabled instances instances in the VPC use DNS resolution

### VPC Subnets
  
- What is a subnet
  - AZ resilient feature of a VPC
  - A sub-network of a VPC - Created within 1 AZ and cannot be changed
    - 1 subnet is in 1 AZ, and a subnet can never be in more than 1 AZ  
    - if the AZ fails so does the subnet and services hosted in the subnet
  - The IPv4 CIDR is a subset of the VPC CIDR
    - Cannot overlap with other subnets
  - Can optionally be given a IPv6 CIDR if its enabled on the VPC `/64` subset of the `/56` VPC (256 /65 Subsets)
  - Subnets can communicate with other subnets in the VPC
- Reserved subnet IP Addresses
  - There are 5 reserved IP Addresses in every subnet
    - `Network` Address - First address in the network
    - `Network +1` Address - VPC Router (Logical Address that moves data between subnets and in/out the VPC)
    - `Network +2` Address - This is used for DNS
    - `Network +3` Address - Reserved for future use
    - `Broadcast` Address - Last IP in the subnet
- DHCP Option Sets
  - 1 DHCP Option set is applied to 1 VPC at a time, this flows to subnets
    - This controls things like DNS,NTP,NetBio,etc.
    - DHCP option sets can be created but not edited
      - If you want to change a setting you need to create a new one then change the VPC allocation to the new one
- 2 Important subnet options
  - These are both set at the subnet level
    - Auto assign public IPv4 addresses
    - Auto assign IPv6 addresses

## Encryption 101

### Approaches to Encryption

Encryption at Rest

- Designed to prevent an attacker from accessing un-encrypted data by insuring that it is encrypted when on a disk
  - This is commonly used in cloud environments since your data is generally hosted on shared hardware, if someone where able to get access to the hardware they would not be able to access the data
- Generally only used when one party is involved

Encryption in Transit

- Data is encrypted at the source, and once it gets to its destination it is decrypted there
  - The raw data is wrapped in an "encryption tunnel" any one looking at the data from the outside sees random bits of jumbled data
  - Generally when multiple individuals or systems are involved

### Encryption Concepts

- Plaintext
  - Any data that is not encrypted (Does not have to be text; can be any data)
- Algorithm
  - A piece of math that takes plaintext and an encryption key and generates encrypted data
    - Requires plaintext and a key to function
- Key
  - Can be as simple as a password, but likely is always more complicated
  - Different types of key ;
    - Symmetric Key ; The person encrypting the data and receiving the data use the same 'key' to encrypt and decrypt the data
    - Asymmetric Key ;  The person encrypting the data uses one key to encrypt the data, the person receiving the data then uses a different key to decrypt it
      - Public/Private key, Only the private key can decrypt, but the public key can encrypt
- Cipher text
  - This is the output when plaintext is put through an algorithm
    - Basically encrypted data

### S3 Object Encryption

- Buckets themselves are not encrypted objects inside them can be encrypted
  - You define encryption at an object level not a bucket level
- S3 Has two types of encryption **At Rest**:
  - Client Side Encryption
    - The object being uploaded are encrypted by the client before they leave (Data is received encrypted)
    - The client owns the Keys, Process, & Tooling
  - Server Side Encryption
    - Data is uploaded in its plaintext form, once the data is received by S3 it is then encrypted by the S3 infrastructure
- Default Bucket Encryption
  - When you upload something to S3 you are using a PUT OBJECT operation as part of this operation you specify a header `x-amz-server-side-encryption`
  - If you do not specify the header no encryption will be used
  - If you do specify the header :
    - `AES256` - SSE-S3 is used
    - `aws:kms` - SSE-KMS is used
  - You have the ability to specify a default for a bucket (What is used when no header is specified)

#### Server Side Encryption

- Types of encryption settings available
  - Server Side Encryption with Customer Provided Keys `SSE-C`
    - The customer is responsible for the keys used for encrypting & decrypting
    - The S3 Service handles the process of encrypting & decrypting
  - Server Side Encryption with Amazon S3-Manager Keys `SSE-S3`
    - S3 is responsible for both the Encryption and Decryption process as well as the key generation and management
    - S3 creates a master key to use, this cannot be seen in the UI and is rotated automatically
    - Each object encrypted with `SSE-S3` its encrypted by a key that is unique for every object
    - S3 generates a key to encrypt an object, the master key is then used to encrypt that key.
  - Server Side Encryption With Customer Master Keys Stored in AWS Key Management Service `SSE-KMS`
    - AWS is responsible for the encryption and decryption as well as the key generation and management.
    - Key Management Server `KMS` handles the master key used to encrypt and decrypt
      - An Amazon managed Customer Master Key `CMS`is generated for us by S3
      - Every time an object is uploaded S3 uses a unique data encryption key to encrypt it
        - The encrypted data encryption key is then stored with that object to be decrypted by the master key
